#!/usr/bin/env python3
"""
KoGPT2 Korean Poetry Generation Fine-tuning Script

This script fine-tunes a KoGPT2 model for Korean poetry generation.
Usage:
    python main.py [--full-finetuning] [--gpu] [--epochs 3] [--lr 5e-5]
"""

import argparse
import sys
import os
import torch
from config import TrainingConfig
from trainer import KoGPT2Trainer
from data_utils import load_and_preprocess_data
from evaluation import evaluate_model, save_evaluation_results

def parse_arguments():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(description='KoGPT2 Poetry Fine-tuning')
    
    # Training strategy
    parser.add_argument('--full-finetuning', action='store_true', default=True,
                      help='Use full fine-tuning (default: True)')
    parser.add_argument('--head-only', action='store_true', default=False,
                      help='Use head-only fine-tuning')
    parser.add_argument('--trainable-layers', type=int, default=0,
                      help='Number of top transformer layers to keep trainable in head-only mode (default: 0)')
    
    # LoRA settings
    parser.add_argument('--lora', action='store_true', default=False,
                      help='Use LoRA (Low-Rank Adaptation) fine-tuning')
    parser.add_argument('--lora-r', type=int, default=32,
                      help='LoRA rank (default: 32)')
    parser.add_argument('--lora-alpha', type=int, default=64,
                      help='LoRA scaling parameter (default: 64)')
    parser.add_argument('--lora-dropout', type=float, default=0.05,
                      help='LoRA dropout rate (default: 0.05)')
    parser.add_argument('--lora-target-modules', nargs='+', default=['c_attn', 'c_proj', 'c_fc'],
                      help='Target modules for LoRA (default: c_attn c_proj c_fc)')
    
    # Prefix-tuning settings
    parser.add_argument('--prefix-tuning', action='store_true', default=False,
                      help='Use Prefix-tuning fine-tuning')
    parser.add_argument('--prefix-length', type=int, default=30,
                      help='Number of prefix tokens (default: 30)')
    parser.add_argument('--prefix-dropout', type=float, default=0.1,
                      help='Prefix dropout rate (default: 0.1)')
    parser.add_argument('--prefix-hidden-size', type=int, default=None,
                      help='Prefix hidden size (default: model hidden size)')
    
    # Prompt-tuning settings
    parser.add_argument('--prompt-tuning', action='store_true', default=False,
                      help='Use Prompt-tuning fine-tuning')
    parser.add_argument('--prompt-length', type=int, default=20,
                      help='Number of prompt tokens (default: 20)')
    parser.add_argument('--prompt-init-method', type=str, default='RANDOM',
                      choices=['RANDOM', 'TEXT'],
                      help='Prompt initialization method (default: RANDOM)')
    
    # Hardware
    parser.add_argument('--gpu', action='store_true', default=True,
                      help='Use GPU if available (default: True)')
    parser.add_argument('--cpu', action='store_true', default=False,
                      help='Force CPU usage')
    
    # A100 optimization
    parser.add_argument('--a100-optimized', action='store_true', default=False,
                      help='Use A100 optimized settings (larger batch size, mixed precision)')
    parser.add_argument('--mixed-precision', action='store_true', default=False,
                      help='Enable mixed precision training')
    
    # Training parameters
    parser.add_argument('--epochs', type=int, default=3,
                      help='Number of training epochs (default: 3)')
    parser.add_argument('--lr', type=float, default=5e-5,
                      help='Learning rate (default: 5e-5)')
    parser.add_argument('--batch-size', type=int, default=4,
                      help='Batch size (default: 4)')
    parser.add_argument('--max-length', type=int, default=512,
                      help='Maximum sequence length (default: 512)')
    
    # Logging
    parser.add_argument('--quiet', action='store_true', default=False,
                      help='Quiet mode with minimal logging')
    
    # Model
    parser.add_argument('--model-name', type=str, default='skt/kogpt2-base-v2',
                      help='Model name or path (default: skt/kogpt2-base-v2)')
    
    # Mode
    parser.add_argument('--mode', type=str, default='train', 
                      choices=['train', 'evaluate', 'generate', 'all'],
                      help='Mode: train, evaluate, generate, or all (default: all)')
    parser.add_argument('--model-path', type=str, default=None,
                      help='Path to saved model for evaluation or generation')
    
    return parser.parse_args()

def main():
    """Main function."""
    print("=== KoGPT2 Korean Poetry Fine-tuning ===\n")
    
    # Parse arguments
    args = parse_arguments()
    
    # Create configuration
    config = TrainingConfig()
    
    # Update config based on arguments
    if args.head_only:
        config.full_finetuning = False
    else:
        config.full_finetuning = args.full_finetuning
    
    config.trainable_layers = args.trainable_layers
    config.learning_rate = args.lr

    
    # LoRA settings
    config.use_lora = args.lora
    config.lora_r = args.lora_r
    config.lora_alpha = args.lora_alpha
    config.lora_dropout = args.lora_dropout
    config.lora_target_modules = args.lora_target_modules
    
    # Prefix-tuning settings
    config.use_prefix_tuning = args.prefix_tuning
    config.prefix_length = args.prefix_length
    config.prefix_dropout = args.prefix_dropout
    config.prefix_hidden_size = args.prefix_hidden_size
    
    # Prompt-tuning settings
    config.use_prompt_tuning = args.prompt_tuning
    config.prompt_length = args.prompt_length
    config.prompt_init_method = args.prompt_init_method
    
    # PEFT techniques optimization
    if config.use_lora:
        # LoRAëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë” ë†’ì€ í•™ìŠµë¥ ì´ í•„ìš”í•¨
        if args.lr == 5e-5:  # ê¸°ë³¸ê°’ì¸ ê²½ìš°ì—ë§Œ ìë™ ì¡°ì •
            config.learning_rate = 5e-4  # 10ë°° ì¦ê°€
            print(f"ğŸ¯ LoRA ìµœì í™”: í•™ìŠµë¥ ì„ {config.learning_rate:.0e}ë¡œ ìë™ ì¦ê°€")
        
        # warmup ë‹¨ê³„ë„ ëŠ˜ë ¤ì„œ ì•ˆì •ì ì¸ í•™ìŠµ
        config.warmup_steps = 200  # 100 -> 200ìœ¼ë¡œ ì¦ê°€
        print(f"ğŸ¯ LoRA ìµœì í™”: warmup stepsë¥¼ {config.warmup_steps}ë¡œ ì¦ê°€")
    
    if config.use_prefix_tuning:
        # Prefix-tuningì€ ë” ë‚®ì€ í•™ìŠµë¥ ì´ ì•ˆì •ì ì„
        if args.lr == 5e-5:  # ê¸°ë³¸ê°’ì¸ ê²½ìš°ì—ë§Œ ìë™ ì¡°ì •
            config.learning_rate = 1e-4  # 2ë°° ì¦ê°€ (LoRAë³´ë‹¤ ë³´ìˆ˜ì )
            print(f"ğŸ¯ Prefix-tuning ìµœì í™”: í•™ìŠµë¥ ì„ {config.learning_rate:.0e}ë¡œ ìë™ ì¦ê°€")
        
        # warmup ë‹¨ê³„ ì¦ê°€
        config.warmup_steps = 150  # 100 -> 150ìœ¼ë¡œ ì¦ê°€
        print(f"ğŸ¯ Prefix-tuning ìµœì í™”: warmup stepsë¥¼ {config.warmup_steps}ë¡œ ì¦ê°€")
    
    if config.use_prompt_tuning:
        # Prompt-tuningì€ Prefixë³´ë‹¤ ë” ë‹¨ìˆœí•˜ë¯€ë¡œ ì•ˆì •ì ì¸ ì„¤ì •
        if args.lr == 5e-5:  # ê¸°ë³¸ê°’ì¸ ê²½ìš°ì—ë§Œ ìë™ ì¡°ì •
            config.learning_rate = 1e-4  # 2ë°° ì¦ê°€ (Prefixì™€ ë™ì¼)
            print(f"ğŸ¯ Prompt-tuning ìµœì í™”: í•™ìŠµë¥ ì„ {config.learning_rate:.0e}ë¡œ ìë™ ì¦ê°€")
        
        # warmup ë‹¨ê³„ëŠ” ê¸°ë³¸ê°’ ìœ ì§€ (ë” ë‹¨ìˆœí•˜ë¯€ë¡œ)
        config.warmup_steps = 100
        print(f"ğŸ¯ Prompt-tuning ìµœì í™”: warmup stepsë¥¼ {config.warmup_steps}ë¡œ ì„¤ì •")
    
    # PEFT ê¸°ë²• ì¶©ëŒ ì²´í¬
    peft_methods = [config.use_lora, config.use_prefix_tuning, config.use_prompt_tuning]
    active_methods = sum(peft_methods)
    
    if active_methods > 1:
        print("âš ï¸ ê²½ê³ : ì—¬ëŸ¬ PEFT ê¸°ë²•ì„ ë™ì‹œì— ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        if config.use_lora:
            print("   LoRAë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            config.use_prefix_tuning = False
            config.use_prompt_tuning = False
        elif config.use_prefix_tuning:
            print("   Prefix-tuningë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            config.use_prompt_tuning = False
    
    if args.cpu:
        config.use_gpu = False
        config.device = 'cpu'
    else:
        config.use_gpu = args.gpu
        
    config.num_epochs = args.epochs
    config.batch_size = args.batch_size
    config.max_length = args.max_length
    config.model_name = args.model_name
    
    # Logging settings
    config.quiet_mode = args.quiet
    
    # A100 optimization settings
    if args.a100_optimized:
        print("ğŸš€ A100 optimization enabled!")
        if config.use_lora:
            config.batch_size = 64  # LoRAëŠ” ë©”ëª¨ë¦¬ë¥¼ ì ê²Œ ì‚¬ìš©í•˜ë¯€ë¡œ ë” í° ë°°ì¹˜ ì‚¬ì´ì¦ˆ ê°€ëŠ¥
            print(f"   - LoRA + A100: Batch size increased to {config.batch_size}")
            # LoRA + A100 ì¡°í•©ì—ì„œëŠ” ë” ì ê·¹ì ì¸ í•™ìŠµë¥  ì ìš©
            if args.lr == 5e-5:  # ê¸°ë³¸ê°’ì¸ ê²½ìš°
                config.learning_rate = 2e-4  # ë” ë†’ì€ í•™ìŠµë¥ 
                print(f"   - LoRA + A100: Learning rate increased to {config.learning_rate:.0e}")
        elif config.use_prefix_tuning:
            config.batch_size = 32  # Prefix-tuningë„ ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
            print(f"   - Prefix-tuning + A100: Batch size increased to {config.batch_size}")
            # Prefix-tuning + A100 ì¡°í•©ì—ì„œëŠ” ë³´ìˆ˜ì ì¸ í•™ìŠµë¥  ì ìš©
            if args.lr == 5e-5:  # ê¸°ë³¸ê°’ì¸ ê²½ìš°
                config.learning_rate = 1.5e-4  # ë³´ìˆ˜ì ì¸ ì¦ê°€
                print(f"   - Prefix-tuning + A100: Learning rate increased to {config.learning_rate:.0e}")
        elif config.use_prompt_tuning:
            config.batch_size = 40  # Prompt-tuningì€ ê°€ì¥ ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
            print(f"   - Prompt-tuning + A100: Batch size increased to {config.batch_size}")
            # Prompt-tuning + A100 ì¡°í•©
            if args.lr == 5e-5:  # ê¸°ë³¸ê°’ì¸ ê²½ìš°
                config.learning_rate = 1.3e-4  # ì ë‹¹í•œ ì¦ê°€
                print(f"   - Prompt-tuning + A100: Learning rate increased to {config.learning_rate:.0e}")
        else:
            config.batch_size = 16  # Increase batch size for A100
            print(f"   - Batch size increased to {config.batch_size}")
        config.use_mixed_precision = True
        config.dataloader_num_workers = 4
        print(f"   - Mixed precision enabled")
        print(f"   - DataLoader workers: {config.dataloader_num_workers}")
    
    if args.mixed_precision:
        config.use_mixed_precision = True
    
    # Store command line arguments in config for saving
    config.command_line_args = vars(args)
    
    # Print configuration
    print("Configuration:")
    print(f"- Model: {config.model_name}")
    print(f"- Device: {config.device}")
    print(f"- Full fine-tuning: {config.full_finetuning}")
    print(f"- Use LoRA: {config.use_lora}")
    if config.use_lora:
        print(f"- LoRA rank: {config.lora_r}")
        print(f"- LoRA alpha: {config.lora_alpha}")
        print(f"- LoRA dropout: {config.lora_dropout}")
        print(f"- LoRA target modules: {config.lora_target_modules}")
    print(f"- Use Prefix-tuning: {config.use_prefix_tuning}")
    if config.use_prefix_tuning:
        print(f"- Prefix length: {config.prefix_length}")
        print(f"- Prefix dropout: {config.prefix_dropout}")
        print(f"- Prefix hidden size: {config.prefix_hidden_size or 'model default'}")
    print(f"- Use Prompt-tuning: {config.use_prompt_tuning}")
    if config.use_prompt_tuning:
        print(f"- Prompt length: {config.prompt_length}")
        print(f"- Prompt initialization: {config.prompt_init_method}")
    print(f"- Epochs: {config.num_epochs}")
    print(f"- Learning rate: {config.learning_rate}")
    print(f"- Batch size: {config.batch_size}")
    print(f"- Max length: {config.max_length}")
    print()
    
    # Load and preprocess data
    print("Loading and preprocessing data...")
    train_data, val_data, test_data = load_and_preprocess_data(config)
    
    # Initialize trainer
    trainer = KoGPT2Trainer(config)
    
    if args.mode in ['train', 'all']:
        print("\n=== Starting Training ===")
        
        # Setup model and tokenizer
        trainer.setup_model_and_tokenizer()
        
        # Train the model
        trainer.train(train_data, val_data)
        
        print("Training completed!")
        
    if args.mode in ['evaluate', 'all']:
        print("\n=== Starting Evaluation ===")
        
        # Load model if path is provided
        if args.model_path:
            trainer.load_model(args.model_path)
        elif not hasattr(trainer, 'model') or trainer.model is None:
            # Load best model from training
            best_model_path = os.path.join(config.model_save_dir, "best_model")
            if os.path.exists(best_model_path):
                trainer.load_model(best_model_path)
            else:
                print("No trained model found. Please run training first or provide --model-path")
                return
        
        # Ensure tokenizer is set up if not already done
        if not hasattr(trainer, 'tokenizer') or trainer.tokenizer is None:
            trainer.setup_model_and_tokenizer()
        
        # Evaluate model
        results = evaluate_model(trainer.model, trainer.tokenizer, test_data, config)
        
        # Save results
        save_evaluation_results(results, config)
        
        print("Evaluation completed!")
        
    if args.mode in ['generate', 'all']:
        print("\n=== Testing Poetry Generation ===")
        
        # Load model if path is provided
        if args.model_path:
            trainer.load_model(args.model_path)
        elif not hasattr(trainer, 'model') or trainer.model is None:
            # Load best model from training
            best_model_path = os.path.join(config.model_save_dir, "best_model")
            if os.path.exists(best_model_path):
                trainer.load_model(best_model_path)
            else:
                print("No trained model found. Please run training first or provide --model-path")
                return
        
        # Ensure tokenizer is set up if not already done
        if not hasattr(trainer, 'tokenizer') or trainer.tokenizer is None:
            trainer.setup_model_and_tokenizer()
        
        # Test poem generation
        test_topics = ["ìì—°", "ì‚¬ë‘", "ê·¸ë¦¬ì›€", "ê°€ì„", "ë‹¬ë¹›"]
        
        print("Generated poems:")
        print("=" * 50)
        
        # Print model and tokenizer info
        print(f"ğŸ¤– Model info:")
        print(f"   Type: {type(trainer.model)}")
        print(f"   Device: {trainer.model.device if hasattr(trainer.model, 'device') else 'Unknown'}")
        print(f"   Embedding size: {trainer.model.get_input_embeddings().num_embeddings}")
        print(f"ğŸ”¤ Tokenizer info:")
        print(f"   Vocab size: {len(trainer.tokenizer)}")
        print(f"   Special tokens: {trainer.tokenizer.special_tokens_map}")
        print("=" * 50)
        
        for topic in test_topics:
            print(f"\nTopic: {topic}")
            print("-" * 30)
            try:
                poem = trainer.generate_poem(topic)
                print(poem)
            except Exception as e:
                print(f"Error generating poem: {e}")
                import traceback
                traceback.print_exc()
            print("-" * 30)
        
        # Interactive generation
        print("\n=== Interactive Generation ===")
        print("Enter topics to generate poems (type 'quit' to exit):")
        
        while True:
            try:
                topic = input("\nEnter topic: ").strip()
                if topic.lower() in ['quit', 'exit', 'q']:
                    break
                
                if topic:
                    print(f"\nGenerating poem for '{topic}'...")
                    poem = trainer.generate_poem(topic)
                    print("\nGenerated poem:")
                    print("-" * 30)
                    print(poem)
                    print("-" * 30)
                    
            except KeyboardInterrupt:
                break
            except Exception as e:
                print(f"Error: {e}")
    
    print("\nAll tasks completed!")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nProcess interrupted by user.")
        sys.exit(1)
    except Exception as e:
        print(f"\nError: {e}")
        sys.exit(1) 